{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e971ad-1052-47d1-adfe-4243b577ad81",
   "metadata": {},
   "source": [
    "## `import` ‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏à‡∏£‡∏ó‡∏¢‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdb1aee9-e241-47b7-b96e-1f74b76b5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pythainlp.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933f065",
   "metadata": {},
   "source": [
    "\n",
    "# üìò ‡∏™‡∏£‡∏∏‡∏õ Regular Expression (Regex) ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢\n",
    "\n",
    "## 1. ‡∏ï‡∏±‡∏ß‡∏ó‡∏≥‡∏ã‡πâ‡∏≥ (Repeaters: `*`, `+`, `{ }`)\n",
    "‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà pattern ‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏î‡∏≠‡∏Å‡∏à‡∏±‡∏ô `*`\n",
    "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 0 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "ab*c\n",
    "\n",
    "**match:**\n",
    "- ac\n",
    "- abc\n",
    "- abbc\n",
    "- abbbc\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ö‡∏ß‡∏Å `+`\n",
    "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏ã‡πâ‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "ab+c\n",
    "\n",
    "**match:**\n",
    "- abc\n",
    "- abbc\n",
    "- abbbc\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏°**\n",
    "(ab)+c\n",
    "\n",
    "**match:**\n",
    "- abc\n",
    "- ababc\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤ `{ }` (‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á)\n",
    "- `{2}` = ‡∏ã‡πâ‡∏≥ 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á  \n",
    "- `{min,}` = ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ min ‡∏Ñ‡∏£‡∏±‡πâ‡∏á  \n",
    "- `{min,max}` = ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ min ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô max\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "ab{2,5}\n",
    "\n",
    "**match:**\n",
    "- abb\n",
    "- abbb\n",
    "- abbbb\n",
    "- abbbbb\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Wildcard ‡∏à‡∏∏‡∏î `.`\n",
    "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ 1 ‡∏ï‡∏±‡∏ß  \n",
    "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö `*` ‚Üí `.*` ‡∏Ñ‡∏∑‡∏≠ ‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    ".*\n",
    "\n",
    "**match:** ‡πÄ‡∏ä‡πà‡∏ô  \n",
    "- abc  \n",
    "- 123!!  \n",
    "- hello world\n",
    "\n",
    "---\n",
    "\n",
    "## 6. ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° `?` (‡∏ó‡∏≥‡πÉ‡∏´‡πâ optional)\n",
    "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏°‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÑ‡∏î‡πâ\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "docx?\n",
    "\n",
    "**match:**  \n",
    "- doc  \n",
    "- docx\n",
    "\n",
    "---\n",
    "\n",
    "## 7. ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ `^` (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)\n",
    "‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤ pattern ‡∏ï‡πâ‡∏≠‡∏á \"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î\"\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "^\\d{3}\n",
    "\n",
    "**match:**  \n",
    "- \"901\" ‡πÉ‡∏ô \"901-333-\"  \n",
    "‚Üí ‡∏à‡∏±‡∏ö‡πÄ‡∏•‡∏Ç 3 ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î\n",
    "\n",
    "---\n",
    "\n",
    "## 8. ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ `$` (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î)\n",
    "‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤ pattern ‡∏ï‡πâ‡∏≠‡∏á \"‡∏à‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î\"\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "-\\d{3}$\n",
    "\n",
    "**match:**  \n",
    "- \"-333\" ‡πÉ‡∏ô \"-901-333\"  \n",
    "‚Üí ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡∏µ‡∏î + ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 3 ‡∏ï‡∏±‡∏ß\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Character Classes (‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞)\n",
    "\n",
    "| ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ |\n",
    "|-----------|-----------|\n",
    "| `\\s` | whitespace (space, tab) |\n",
    "| `\\S` | ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà whitespace |\n",
    "| `\\d` | ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0‚Äì9 |\n",
    "| `\\D` | ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç |\n",
    "| `\\w` | word character (A‚ÄìZ, a‚Äìz, 0‚Äì9, _) |\n",
    "| `\\W` | non-word character |\n",
    "| `\\b` | word boundary |\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "\\w+\n",
    "\n",
    "match ‡∏Ñ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô `hello123`\n",
    "\n",
    "---\n",
    "\n",
    "## 10. ‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏î‡πâ‡∏ß‡∏¢ `[^ ]` (NOT)\n",
    "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏ï‡∏±‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "[^abc]\n",
    "\n",
    "match ‡∏ï‡∏±‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô `a`, `b`, `c`\n",
    "\n",
    "---\n",
    "\n",
    "## 11. ‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ `[a-z]`\n",
    "‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "[a-zA-Z]\n",
    "\n",
    "match ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß (‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å/‡πÉ‡∏´‡∏ç‡πà)\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Escape Symbol `\\`\n",
    "‡πÉ‡∏ä‡πâ escape ‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡πÄ‡∏®‡∏©\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "\\d+[+-*]\\d+\n",
    "\n",
    "match:\n",
    "- 2+2  \n",
    "- 3*9  \n",
    "- 10-5  \n",
    "\n",
    "‡πÉ‡∏ô `(2+2) * 3*9` ‡∏Å‡πá match\n",
    "\n",
    "---\n",
    "\n",
    "## 13. ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö `( )`\n",
    "‡πÉ‡∏ä‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° pattern\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "([A-Z]\\w+)\n",
    "\n",
    "match ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà ‡πÄ‡∏ä‡πà‡∏ô `Hello`, `User123`\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Vertical Bar `|` (OR)\n",
    "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏´‡∏£‡∏∑‡∏≠\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "th (e|is|at)\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "match:\n",
    "- the  \n",
    "- this  \n",
    "- that\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98f9e0",
   "metadata": {},
   "source": [
    "# Tokenization /Segmentation\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\"I love machine learning.\"**\n",
    "[\"I\", \"love\", \"machine\", \"learning\", \".\"]\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á \"‡∏â‡∏±‡∏ô‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏£‡πà‡∏≠‡∏¢\"**\n",
    "[\"‡∏â‡∏±‡∏ô\",\"‡∏Å‡∏¥‡∏ô\",\"‡∏Ç‡πâ‡∏≤‡∏ß\",\"‡∏£‡πâ‡∏≤‡∏ô\",\"‡∏≠‡∏£‡πà‡∏≠‡∏¢\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97b638b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT1 = \"\"\"\n",
    "\"<p>‡∏ô‡∏µ‡πâ you # love me? 5555 ‡∏£‡∏±‡∏Å msu: http://www.msu.ac.th/ <html> I love you <a> https://colab.research.google.com/drive/1k0jeP-CEbfNhYqxwDO4O4kI877Rdt_Ad?usp=sharing#scrollTo=CPX885Sx4aOh so much\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "666f80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT2 = \"\"\"\n",
    "‡∏£‡∏±‡∏Å love ‡∏â‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÄ‡∏ò‡∏≠ 12346 ‡∏â‡∏±‡∏ô 25 you ‡∏ô ‡∏°‡∏µ‡πÉ‡∏Ñ‡∏£ ‡∏£ ‡∏£ ‡πÑ‡∏´‡∏°\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "740d0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag_html(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '',text)\n",
    "\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'http\\S+|www\\S+')\n",
    "    return re.sub(url, '', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_only_english(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "def tokenization_english(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2168736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_only_thai(text):\n",
    "    return re.sub(r'[^\\u0E00-\\u0E7F\\s]', '', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_alone_characters(text):\n",
    "    tokens = word_tokenize(text,engine='longest', keep_whitespace=False)\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def tokenization_thai(text):\n",
    "    token = word_tokenize(text,engine='longest', keep_whitespace=False)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f36aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: ['you', 'love', 'me', 'msu', 'I', 'love', 'you', 'so', 'much']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    INPUT_TEXT1 = remove_tag_html(INPUT_TEXT1)\n",
    "    INPUT_TEXT1 = remove_url(INPUT_TEXT1)\n",
    "    INPUT_TEXT1 = remove_punctuation(INPUT_TEXT1)\n",
    "    INPUT_TEXT1 = remove_only_english(INPUT_TEXT1)\n",
    "    INPUT_TEXT1 = tokenization_english(INPUT_TEXT1)\n",
    "    print(\"Final:\", INPUT_TEXT1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "441ce4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: ['‡∏£‡∏±‡∏Å', '‡∏â‡∏±‡∏ô', '‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤', '‡πÄ‡∏ò‡∏≠', '‡∏â‡∏±‡∏ô', '‡∏°‡∏µ', '‡πÉ‡∏Ñ‡∏£', '‡πÑ‡∏´‡∏°']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    INPUT_TEXT2 = remove_only_thai(INPUT_TEXT2)\n",
    "    INPUT_TEXT2 = remove_numbers(INPUT_TEXT2)\n",
    "    INPUT_TEXT2 = remove_alone_characters(INPUT_TEXT2)\n",
    "    INPUT_TEXT2 = tokenization_thai(INPUT_TEXT2)\n",
    "    print(\"Final:\", INPUT_TEXT2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
